# -*- coding: utf-8 -*-
"""Untitled28.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KjUD20Uj5qxfbTUxCfuidYC4k93cecje
"""

# similarity_join.py
import re
import pandas as pd
import numpy as np
from ast import literal_eval

class SimilarityJoin:
    def __init__(self, data_file1, data_file2):
        self.df1 = pd.read_csv(data_file1)
        self.df2 = pd.read_csv(data_file2)
          
    def preprocess_df(self, df, cols): 
        """
            Write your code!
        """ 
        df[cols[1]]=[str(x).replace('.','').replace(')','') for x in df[cols[1]]]
        df[cols[1]]=[str(x).replace('nan','') for x in df[cols[1]]]
        df_process=pd.DataFrame({'{}'.format(cols[0]):df[cols[0]],'{}'.format(cols[1]):df[cols[1]]})
        df_process["concat_text"]=df["{}".format(cols[0])]+" "+df[cols[1]].fillna('')
        df_process["tokens_"]=[re.split(r'\W+',str(x.strip())) for x in df_process["concat_text"]]
        df_process["tokens_"]=[str(x).strip().lower() for x in df_process["tokens_"]]
        df_process.drop("concat_text",1,inplace=True)
        df["joinKey"]=df_process["tokens_"]
        return df
    
    def filtering(self, df1, df2):
        """
            Write your code!
        """
        df1=df1[['id','joinKey']]
        df2=df2[['id','joinKey']]
        df1.columns=['id1', 'joinKey']
        df2.columns=['id2', 'joinKey']
        df1['joinKey1']=df1['joinKey']
        df2["joinKey2"]=df2['joinKey']
        
        df1['joinKey'] = df1['joinKey'].apply(literal_eval) 
        df1=df1.explode('joinKey')
        df1=df1.drop_duplicates()

        df2['joinKey'] = df2['joinKey'].apply(literal_eval) 
        df2=df2.explode('joinKey')
        df2=df2.drop_duplicates()
        
        df_merge=df1.merge(df2,how='inner',on=['joinKey'])
        df_merge.to_csv("df3.csv")
        df_merge["combined_id"]=df_merge["id1"]+df_merge["id2"]
        df_group=df_merge.groupby('combined_id',group_keys=False,as_index=False)
        df_group=df_group.first()
        df_group=df_group.drop(columns=["joinKey","combined_id"])
        return df_group

    def verification(self, cand_df, threshold):
        """
            Write your code!
        """
        list1=[]
        for i in range(len(cand_df)):
          row=cand_df.loc[i]
          x=set((row["joinKey1"]).replace("[","").replace("]","").replace("'",'').split(", "))
          y=set((row["joinKey2"]).replace("[","").replace("]","").replace("'",'').split(", "))
          intersection = list(x & y)
          union = list(x | y)
          list1.append(len(intersection)/len(union))
       
        cand_df["jaccard"]=[float(x) for x in list1]
        cand_df=cand_df[cand_df["jaccard"]>threshold]
        return cand_df
         
    def evaluate(self, result, ground_truth):
        """
            Write your code!
        """
        result=[item for sublist in result for item in sublist]
        ground_truth=[item for sublist in ground_truth for item in sublist]

        r=len(list(set(result) & set(ground_truth)))
        recall=r/len(ground_truth)
        precision=r/len(result)

        fmeasure=(2*precision*recall)/(precision+recall)
        return (precision, recall, fmeasure)
        
    def jaccard_join(self, cols1, cols2, threshold):
        new_df1 = self.preprocess_df(self.df1, cols1)
        new_df2 = self.preprocess_df(self.df2, cols2)
        print ("Before filtering: %d pairs in total" %(self.df1.shape[0] *self.df2.shape[0])) 
        
        cand_df = self.filtering(new_df1, new_df2)
        print ("After Filtering: %d pairs left" %(cand_df.shape[0]))
        
        result_df = self.verification(cand_df, threshold)
        print ("After Verification: %d similar pairs" %(result_df.shape[0]))
        
        return result_df
       
        

if __name__ == "__main__":
    er = SimilarityJoin("Amazon_sample.csv", "Google_sample.csv")
    amazon_cols = ["title", "manufacturer"]
    google_cols = ["name", "manufacturer"]
    result_df = er.jaccard_join(amazon_cols, google_cols, 0.5)

    result = result_df[['id1', 'id2']].values.tolist()
    ground_truth = pd.read_csv("Amazon_Google_perfectMapping_sample.csv").values.tolist()
    print ("(precision, recall, fmeasure) = ", er.evaluate(result, ground_truth))

